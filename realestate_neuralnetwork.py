# -*- coding: utf-8 -*-
"""RealEstate-NeuralNetwork.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/18XlyepeP_1t5fAUz8V0393TLBO4ee54e

References:
Code is based on the learing from here

 https://www.youtube.com/watch?v=vSzou5zRwNQ&feature=youtu.be
and
 https://randerson112358.medium.com/predict-house-median-prices-5f1a768dd256 

Developed based on information at 
https://machinelearningmastery.com/regression-tutorial-keras-deep-learning-library-python/
"""

!pip install tensorflow

# We are using Pandas, Sklearn, Keras and Tensor Flow for 
# this neural network Project. 
import pandas as pd
import numpy
from sklearn import preprocessing

from sklearn.model_selection import train_test_split
from keras.models import Sequential
from keras.layers import Dense
import matplotlib.pyplot as plt
from google.colab import files

uploaded = files.upload()
# The data contains 1183 Records in total
# it contains the bedroom of the Flat, Carpet Area of the Flat, 
# Number of Parking space alloted to Flat,
# Number of Bathrooms and the Price in Lakhs
# bedroom, carpet_area, balcony, park_count,bath, price_in_Lac 
# Each of the values would be considered as a Feature in the Neural Network 
df = pd.read_csv('Wo_Catg_var_Data_For_NeuralNetwork.csv') 
print("Start Data Frame-------------------->")
print(df)
print("<------------------------Data Frame End")
dataset = df.values

#Since each of the columns has a different range of Value
#The columns would have to scaled  in between  0 and 1 
min_max_scaler = preprocessing.MinMaxScaler()
dataset_scale = min_max_scaler.fit_transform(dataset)

# The Dataset is Split into Dependent Variables X and 
# the Independent/ Prediction Variable Y 
X_scale = dataset_scale[:,0:5]
Y_scale = dataset_scale[:,5]

print("X -------------------->")
X_scale
print("<-------------------- X")
print("Y -------------------->")
Y_scale
print("<-------------------- Y")
Y_scale

#Total Dataset Split into 70% for Training ie X_train dataset
X_train, X_val_and_test, Y_train, Y_val_and_test = train_test_split(X_scale, Y_scale, test_size=0.3)

#The remaining dataset is split into 15% for validation and 15% for testing
X_val, X_test, Y_val, Y_test = train_test_split(X_val_and_test, Y_val_and_test, test_size=0.5)

print("Shapes -------------------->")
print(X_train.shape, X_val.shape, X_test.shape, Y_train.shape, Y_val.shape, Y_test.shape)
print("<-------------------- Shapes")
X_train

# Model the Architecture of the Neural Network
# Number of Input Features  = 5
# Number of Hidden Layer is 1
# Number of nodes in the layer is 13
# Number of Output nodes = 1 
# Activation function in teh input and Hidden layer is Rectified Linear Unit (ReLU)

model = Sequential()
model.add(Dense(13, input_dim=5, kernel_initializer='normal', activation='relu'))
model.add(Dense(1, kernel_initializer='normal'))

# Compile model
#Loss function is mentioned as Mean Squared Error 
#This loss functtion is generally used for prediction of Regression Problems
# Referance : https://machinelearningmastery.com/how-to-choose-loss-functions-when-training-deep-learning-neural-networks/  

#Loss function is for Measuring  how  good is the Traininig 
#Optimizer improves the Training efficiency. Function used here is adam optimizer (sgd)
# adam optimizer :  https://machinelearningmastery.com/adam-optimization-algorithm-for-deep-learning/#:~:text=Adam%20is%20a%20replacement%20optimization,sparse%20gradients%20on%20noisy%20problems. 

model.compile(loss='mean_squared_error', optimizer='adam')

# Training the Model done below.
# Run for 100 iterations in batches of 10
hist = model.fit(X_train, Y_train,
          batch_size=50, epochs=100,
          validation_data=(X_val, Y_val))

# Evaluate the Model for Accuracy
model.evaluate(X_test, Y_test)

# Make Predictions using the Model. 
# Use the X_test dataset as input to the prediction
predict = model.predict(X_test)

for y, p in zip(Y_test, predict):
    print("Y=%s,\t\t Predicted=%s, \t\t Error=%s" % (y, p[0], abs(y-p[0])))