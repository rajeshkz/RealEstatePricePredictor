{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3",
      "language": "python"
    },
    "language_info": {
      "name": "python",
      "version": "3.6.11",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    },
    "colab": {
      "name": "RealEstate-NeuralNetwork.ipynb",
      "provenance": [],
      "collapsed_sections": []
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DDrEIslpLRpr"
      },
      "source": [
        "References:\r\n",
        "Code is based on the learing from here\r\n",
        "\r\n",
        " https://www.youtube.com/watch?v=vSzou5zRwNQ&feature=youtu.be\r\n",
        "and\r\n",
        " https://randerson112358.medium.com/predict-house-median-prices-5f1a768dd256 \r\n",
        "\r\n",
        "Developed based on information at \r\n",
        "https://machinelearningmastery.com/regression-tutorial-keras-deep-learning-library-python/ "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "Li8pFNEqGDlt"
      },
      "source": [
        "!pip install tensorflow\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "GWOR8sGFGDl4"
      },
      "source": [
        "# We are using Pandas, Sklearn, Keras and Tensor Flow for \n",
        "# this neural network Project. \n",
        "import pandas as pd\n",
        "import numpy\n",
        "from sklearn import preprocessing\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense\n",
        "import matplotlib.pyplot as plt\n",
        "from google.colab import files\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2JDQxk7lHaNP"
      },
      "source": [
        "uploaded = files.upload()\r\n",
        "# The data contains 1183 Records in total\r\n",
        "# it contains the bedroom of the Flat, Carpet Area of the Flat, \r\n",
        "# Number of Parking space alloted to Flat,\r\n",
        "# Number of Bathrooms and the Price in Lakhs\r\n",
        "# bedroom, carpet_area, balcony, park_count,bath, price_in_Lac \r\n",
        "# Each of the values would be considered as a Feature in the Neural Network \r\n",
        "df = pd.read_csv('Wo_Catg_var_Data_For_NeuralNetwork.csv') \r\n",
        "print(\"Start Data Frame-------------------->\")\r\n",
        "print(df)\r\n",
        "print(\"<------------------------Data Frame End\")\r\n",
        "dataset = df.values\r\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "Io-vhfyeGDl6"
      },
      "source": [
        "#Since each of the columns has a different range of Value\n",
        "#The columns would have to scaled  in between  0 and 1 \n",
        "min_max_scaler = preprocessing.MinMaxScaler()\n",
        "dataset_scale = min_max_scaler.fit_transform(dataset)\n",
        "\n",
        "# The Dataset is Split into Dependent Variables X and \n",
        "# the Independent/ Prediction Variable Y \n",
        "X_scale = dataset_scale[:,0:5]\n",
        "Y_scale = dataset_scale[:,5]\n",
        "\n",
        "print(\"X -------------------->\")\n",
        "X_scale\n",
        "print(\"<-------------------- X\")\n",
        "print(\"Y -------------------->\")\n",
        "Y_scale\n",
        "print(\"<-------------------- Y\")\n",
        "Y_scale"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "5xK6vFW8GDl6"
      },
      "source": [
        "#Total Dataset Split into 70% for Training ie X_train dataset\n",
        "X_train, X_val_and_test, Y_train, Y_val_and_test = train_test_split(X_scale, Y_scale, test_size=0.3)\n",
        "\n",
        "#The remaining dataset is split into 15% for validation and 15% for testing\n",
        "X_val, X_test, Y_val, Y_test = train_test_split(X_val_and_test, Y_val_and_test, test_size=0.5)\n",
        "\n",
        "print(\"Shapes -------------------->\")\n",
        "print(X_train.shape, X_val.shape, X_test.shape, Y_train.shape, Y_val.shape, Y_test.shape)\n",
        "print(\"<-------------------- Shapes\")\n",
        "X_train"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "7r6NI8N7GDl8"
      },
      "source": [
        "# Model the Architecture of the Neural Network\n",
        "# Number of Input Features  = 5\n",
        "# Number of Hidden Layer is 1\n",
        "# Number of nodes in the layer is 13\n",
        "# Number of Output nodes = 1 \n",
        "# Activation function in teh input and Hidden layer is Rectified Linear Unit (ReLU)\n",
        "\n",
        "model = Sequential()\n",
        "model.add(Dense(13, input_dim=5, kernel_initializer='normal', activation='relu'))\n",
        "model.add(Dense(1, kernel_initializer='normal'))\n",
        "\n",
        "# Compile model\n",
        "#Loss function is mentioned as Mean Squared Error \n",
        "#This loss functtion is generally used for prediction of Regression Problems\n",
        "# Referance : https://machinelearningmastery.com/how-to-choose-loss-functions-when-training-deep-learning-neural-networks/  \n",
        "\n",
        "#Loss function is for Measuring  how  good is the Traininig \n",
        "#Optimizer improves the Training efficiency. Function used here is adam optimizer (sgd)\n",
        "# adam optimizer :  https://machinelearningmastery.com/adam-optimization-algorithm-for-deep-learning/#:~:text=Adam%20is%20a%20replacement%20optimization,sparse%20gradients%20on%20noisy%20problems. \n",
        "\n",
        "model.compile(loss='mean_squared_error', optimizer='adam')\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "mC4PpgUfGDl9"
      },
      "source": [
        "# Training the Model done below.\n",
        "# Run for 100 iterations in batches of 10\n",
        "hist = model.fit(X_train, Y_train,\n",
        "          batch_size=50, epochs=100,\n",
        "          validation_data=(X_val, Y_val))\n",
        "\n",
        "# Evaluate the Model for Accuracy\n",
        "model.evaluate(X_test, Y_test)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JZgOK7L3SQ2j"
      },
      "source": [
        "# Make Predictions using the Model. \r\n",
        "# Use the X_test dataset as input to the prediction\r\n",
        "predict = model.predict(X_test)\r\n",
        "\r\n",
        "for y, p in zip(Y_test, predict):\r\n",
        "    print(\"Y=%s,\\t\\t Predicted=%s, \\t\\t Error=%s\" % (y, p[0], abs(y-p[0])))\r\n"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}